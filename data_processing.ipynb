{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "thesising_data_processing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iSknVR21ZJHt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txBu9cl_VDuN"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mlTSAn4VLxH"
      },
      "source": [
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8HRuNuIUKCs"
      },
      "source": [
        "!pip install path.py\n",
        "!pip install pytorch3d\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGZivZ4ATpOe"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from path import Path\n",
        "\n",
        "random.seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fMK1k61UPNC",
        "outputId": "1fed9189-cbf7-459e-e12f-b83021fb3282"
      },
      "source": [
        "!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
        "!unzip -q ModelNet10.zip\n",
        "\n",
        "path = Path(\"ModelNet10\")\n",
        "\n",
        "folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n",
        "\n",
        "clear_output()\n",
        "classes = {folder: i for i, folder in enumerate(folders)}\n",
        "classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bathtub': 0,\n",
              " 'bed': 1,\n",
              " 'chair': 2,\n",
              " 'desk': 3,\n",
              " 'dresser': 4,\n",
              " 'monitor': 5,\n",
              " 'night_stand': 6,\n",
              " 'sofa': 7,\n",
              " 'table': 8,\n",
              " 'toilet': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRaLWTASLYYk"
      },
      "source": [
        "def default_transforms():\n",
        "    return transforms.Compose([\n",
        "                               PointSampler(1024),\n",
        "                               Normalize(),\n",
        "                               RandomNoise(),\n",
        "                               ToSorted(),\n",
        "                               ToTensor()\n",
        "    ])\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1CVwVxdfUfP6TRcVUjjJvQeRcgCGcnSO_\n",
        "from helping import *\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSknVR21ZJHt"
      },
      "source": [
        "### Data Preprocessing (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n65eO9M5UxZJ"
      },
      "source": [
        "with open(path/\"dresser/train/dresser_0001.off\", 'r') as f:\n",
        "    verts, faces = read_off(f)\n",
        "\n",
        "i, j, k = np.array(faces).T\n",
        "x, y, z = np.array(verts).T\n",
        "\n",
        "# len(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH_zTKlzVHuu"
      },
      "source": [
        "# visualize_rotate([go.Mesh3d(x=x, y=y, z=z, color='lightpink', opacity=0.50, i=i,j=j,k=k)]).show()\n",
        "# visualize_rotate([go.Scatter3d(x=x, y=y, z=z, mode='markers')]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "421MhN3xWqp7"
      },
      "source": [
        "# pcshow(x, y, z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIrCMGnhZYfm"
      },
      "source": [
        "pointcloud = PointSampler(1024)((verts, faces))\n",
        "# pcshow(*pointcloud.T)\n",
        "\n",
        "norm_pointcloud = Normalize()(pointcloud)\n",
        "# pcshow(*norm_pointcloud.T)\n",
        "\n",
        "noisy_pointcloud = RandomNoise()(norm_pointcloud)\n",
        "# pcshow(*noisy_pointcloud.T)\n",
        "\n",
        "rot_pointcloud = RandomRotation_z()(noisy_pointcloud)\n",
        "# pcshow(*rot_pointcloud.T)\n",
        "\n",
        "sorted_pointcloud = ToSorted()(rot_pointcloud)\n",
        "# pcshow(*sorted_pointcloud.T)\n",
        "\n",
        "tensor_pointcloud = ToTensor()(sorted_pointcloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FpUXfFhh4km"
      },
      "source": [
        "### Creating Loaders for Final Progress Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHnDH_o8k7md"
      },
      "source": [
        "#### Redefine classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YXhmVHtk3El"
      },
      "source": [
        "class PointCloudData(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms(), folders=None):\n",
        "        self.root_dir = root_dir\n",
        "        if not folders:\n",
        "            folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform\n",
        "        self.valid = valid\n",
        "        self.pcs = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    with open(new_dir/file, 'r') as f:\n",
        "                        verts, faces = read_off(f)\n",
        "                    sample['pc'] = (verts, faces)\n",
        "                    sample['category'] = category\n",
        "                    self.pcs.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pcs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pointcloud = self.transforms(self.pcs[idx]['pc'])\n",
        "        category = self.pcs[idx]['category']\n",
        "        return pointcloud, self.classes[category]\n",
        "    \n",
        "class PointCloudDataPre(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms(), folders=None):\n",
        "        self.root_dir = root_dir\n",
        "        if not folders:\n",
        "            folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.transforms = transform\n",
        "        self.valid = valid\n",
        "        self.pcs = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    with open(new_dir/file, 'r') as f:\n",
        "                        verts, faces = read_off(f)\n",
        "                    sample['pc'] = self.transforms((verts, faces))\n",
        "                    sample['category'] = category\n",
        "                    self.pcs.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pcs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pointcloud = self.pcs[idx]['pc']\n",
        "        category = self.pcs[idx]['category']\n",
        "        return pointcloud, self.classes[category]\n",
        "    \n",
        "    \n",
        "class PointCloudDataBoth(Dataset):\n",
        "    def __init__(self, root_dir, valid=False, folder=\"train\", static_transform=default_transforms(), later_transform=None, folders=None):\n",
        "        self.root_dir = root_dir\n",
        "        if not folders:\n",
        "            folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
        "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
        "        self.static_transform = static_transform\n",
        "        self.later_transform = later_transform\n",
        "        self.valid = valid\n",
        "        self.pcs = []\n",
        "        for category in self.classes.keys():\n",
        "            new_dir = root_dir/Path(category)/folder\n",
        "            for file in os.listdir(new_dir):\n",
        "                if file.endswith('.off'):\n",
        "                    sample = {}\n",
        "                    with open(new_dir/file, 'r') as f:\n",
        "                        verts, faces = read_off(f)\n",
        "                    sample['pc'] = self.static_transform((verts, faces))\n",
        "                    sample['category'] = category\n",
        "                    self.pcs.append(sample)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pcs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pointcloud = self.pcs[idx]['pc']\n",
        "        if self.later_transform is not None:\n",
        "            pointcloud = self.later_transform(pointcloud)\n",
        "        category = self.pcs[idx]['category']\n",
        "        return pointcloud, self.classes[category]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRO-3QhmlTZO"
      },
      "source": [
        "!mkdir drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLz3zao-h-QH"
      },
      "source": [
        "#### Overfitting - all augmentations applied before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA0JVMMph9I-",
        "outputId": "bcca57b0-50cc-4c82-841f-b5f95c4d57e2"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "trs = transforms.Compose([\n",
        "                        PointSampler(1024),\n",
        "                        ToSorted(),\n",
        "                        Normalize(),\n",
        "                        ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataPre(path, folders=['bed'], transform=trs)\n",
        "beds_valid_dataset = PointCloudDataPre(path, folder='test', folders=['bed'], transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_pre\n",
        "torch.save(beds_train_loader, 'dataloader_beds_pre/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_pre/validloader.pth')\n",
        "\n",
        "!mkdir drive/MyDrive/Thesis/dataloaders/final\n",
        "!cp -r dataloader_beds_pre drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_pre’: File exists\n",
            "mkdir: cannot create directory ‘drive/MyDrive/Thesis/dataloaders/final’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4bInGr3izAx"
      },
      "source": [
        "#### Underfitting - all augmentations applied during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvoVxscDiruZ",
        "outputId": "089b5cb8-c73f-4050-b57d-a3b7adcca204"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "trs = transforms.Compose([\n",
        "                               PointSampler(1024),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "                               RandomNoise(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudData(path, folders=['bed'], transform=trs)\n",
        "beds_valid_dataset = PointCloudData(path, folder='test', folders=['bed'], transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, num_workers=4, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_dur\n",
        "torch.save(beds_train_loader, 'dataloader_beds_dur/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_dur/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_dur drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_dur’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "---1er30lBbo"
      },
      "source": [
        "#### Both - static and dynamic transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0zjEopWlBGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2089d8-d904-47d9-e421-1354a00fe90d"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "static_trs = transforms.Compose([\n",
        "                               PointSampler(1024),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "])\n",
        "\n",
        "dynamic_trs = transforms.Compose([\n",
        "                               RandomNoise(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataBoth(path, folders=['bed'], static_transform=static_trs, later_transform=dynamic_trs)\n",
        "beds_valid_dataset = PointCloudDataBoth(path, folder='test', folders=['bed'], static_transform=static_trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_both\n",
        "torch.save(beds_train_loader, 'dataloader_beds_both/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_both/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_both drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_both’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyBuYh9zmK7N"
      },
      "source": [
        "#### Two classes: beds and tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYevb3zsmPGG"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "static_trs = transforms.Compose([\n",
        "                               PointSampler(1024),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "])\n",
        "\n",
        "dynamic_trs = transforms.Compose([\n",
        "                               RandomNoise(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataBoth(path, folders=['bed', 'table'], static_transform=static_trs, later_transform=dynamic_trs)\n",
        "beds_valid_dataset = PointCloudDataBoth(path, folder='test', folders=['bed', 'table'], static_transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_tables\n",
        "torch.save(beds_train_loader, 'dataloader_beds_tables/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_tables/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_tables drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wAY9vCQ9OI"
      },
      "source": [
        "### For 512"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAejWCZtQ6vp"
      },
      "source": [
        "!mkdir drive/MyDrive/Thesis/dataloaders/final512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljEKe3XxQ6wB"
      },
      "source": [
        "#### Overfitting - all augmentations applied before training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvvXjrC6Q6wC",
        "outputId": "3cbd6fba-cfcd-4e08-d7fb-a85358c51522"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "trs = transforms.Compose([\n",
        "                        PointSampler(512),\n",
        "                        ToSorted(),\n",
        "                        Normalize(),\n",
        "                        ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataPre(path, folders=['bed'], transform=trs)\n",
        "beds_valid_dataset = PointCloudDataPre(path, folder='test', folders=['bed'], transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_pre\n",
        "torch.save(beds_train_loader, 'dataloader_beds_pre/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_pre/validloader.pth')\n",
        "\n",
        "!mkdir drive/MyDrive/Thesis/dataloaders/final\n",
        "!cp -r dataloader_beds_pre drive/MyDrive/Thesis/dataloaders/final512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_pre’: File exists\n",
            "mkdir: cannot create directory ‘drive/MyDrive/Thesis/dataloaders/final’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F1r7yCqQ6wC"
      },
      "source": [
        "#### Underfitting - all augmentations applied during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFS4GYuQQ6wC",
        "outputId": "2cff81eb-f660-468c-8e15-530969c8bffe"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "trs = transforms.Compose([\n",
        "                               PointSampler(512),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudData(path, folders=['bed'], transform=trs)\n",
        "beds_valid_dataset = PointCloudData(path, folder='test', folders=['bed'], transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, num_workers=4, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_dur\n",
        "torch.save(beds_train_loader, 'dataloader_beds_dur/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_dur/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_dur drive/MyDrive/Thesis/dataloaders/final512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n",
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_dur’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgjkAOcmQ6wD"
      },
      "source": [
        "#### Both - static and dynamic transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osaVKzmHQ6wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7548786a-a725-4542-82f5-1121c629c2d5"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "static_trs = transforms.Compose([\n",
        "                               PointSampler(512),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "])\n",
        "\n",
        "dynamic_trs = transforms.Compose([\n",
        "                               RandomNoise(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataBoth(path, folders=['bed'], static_transform=static_trs, later_transform=dynamic_trs)\n",
        "beds_valid_dataset = PointCloudDataBoth(path, folder='test', folders=['bed'], static_transform=static_trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_both\n",
        "torch.save(beds_train_loader, 'dataloader_beds_both/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_both/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_both drive/MyDrive/Thesis/dataloaders/final512"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataloader_beds_both’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkb1bFdiQ6wD"
      },
      "source": [
        "#### Two classes: beds and tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INFgDlceQ6wD"
      },
      "source": [
        "BATCH_SIZE = 48\n",
        "\n",
        "static_trs = transforms.Compose([\n",
        "                               PointSampler(512),\n",
        "                               ToSorted(),\n",
        "                               Normalize(),\n",
        "])\n",
        "\n",
        "dynamic_trs = transforms.Compose([\n",
        "                               RandomNoise(),\n",
        "                               ToTensor()\n",
        "])\n",
        "\n",
        "beds_train_dataset = PointCloudDataBoth(path, folders=['bed', 'table'], static_transform=static_trs, later_transform=dynamic_trs)\n",
        "beds_valid_dataset = PointCloudDataBoth(path, folder='test', folders=['bed', 'table'], static_transform=trs)\n",
        "\n",
        "beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
        "beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n",
        "\n",
        "!mkdir dataloader_beds_tables\n",
        "torch.save(beds_train_loader, 'dataloader_beds_tables/trainloader.pth')\n",
        "torch.save(beds_valid_loader, 'dataloader_beds_tables/validloader.pth')\n",
        "\n",
        "!cp -r dataloader_beds_tables drive/MyDrive/Thesis/dataloaders/final"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}