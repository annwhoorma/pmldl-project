{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"thesising_data_processing.ipynb","provenance":[{"file_id":"1_-EU3OgSux9a0lA-LYQfY2USQb5P2gi9","timestamp":1635492917599}],"collapsed_sections":["iSknVR21ZJHt"],"mount_file_id":"1fPX5ewWtcAOpdh_lJMu-JdT622uahDR8","authorship_tag":"ABX9TyOkJQsoRZ4v94pTGClmClHx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"txBu9cl_VDuN"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"5mlTSAn4VLxH"},"source":["from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8HRuNuIUKCs"},"source":["!pip install path.py\n","!pip install pytorch3d\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGZivZ4ATpOe"},"source":["import numpy as np\n","import math\n","import random\n","import os\n","import plotly.graph_objects as go\n","import plotly.express as px\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torchvision import transforms, utils\n","\n","from path import Path\n","\n","random.seed = 42"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fMK1k61UPNC","executionInfo":{"status":"ok","timestamp":1637604588332,"user_tz":-180,"elapsed":27973,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"00bd70ce-e3a5-46bb-e6d0-ab3d64438971"},"source":["!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n","!unzip -q ModelNet10.zip\n","\n","path = Path(\"ModelNet10\")\n","\n","folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n","\n","clear_output()\n","classes = {folder: i for i, folder in enumerate(folders)}\n","classes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bathtub': 0,\n"," 'bed': 1,\n"," 'chair': 2,\n"," 'desk': 3,\n"," 'dresser': 4,\n"," 'monitor': 5,\n"," 'night_stand': 6,\n"," 'sofa': 7,\n"," 'table': 8,\n"," 'toilet': 9}"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"pRaLWTASLYYk"},"source":["def default_transforms():\n","    return transforms.Compose([\n","                               PointSampler(1024),\n","                               Normalize(),\n","                               RandomNoise(),\n","                               ToSorted(),\n","                               ToTensor()\n","    ])\n","\n","!gdown https://drive.google.com/uc?id=1CVwVxdfUfP6TRcVUjjJvQeRcgCGcnSO_\n","from helping import *\n","clear_output()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSknVR21ZJHt"},"source":["### Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"n65eO9M5UxZJ"},"source":["with open(path/\"dresser/train/dresser_0001.off\", 'r') as f:\n","    verts, faces = read_off(f)\n","\n","i, j, k = np.array(faces).T\n","x, y, z = np.array(verts).T\n","\n","# len(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sH_zTKlzVHuu"},"source":["# visualize_rotate([go.Mesh3d(x=x, y=y, z=z, color='lightpink', opacity=0.50, i=i,j=j,k=k)]).show()\n","# visualize_rotate([go.Scatter3d(x=x, y=y, z=z, mode='markers')]).show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"421MhN3xWqp7"},"source":["# pcshow(x, y, z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uIrCMGnhZYfm"},"source":["pointcloud = PointSampler(1024)((verts, faces))\n","# pcshow(*pointcloud.T)\n","\n","norm_pointcloud = Normalize()(pointcloud)\n","# pcshow(*norm_pointcloud.T)\n","\n","noisy_pointcloud = RandomNoise()(norm_pointcloud)\n","# pcshow(*noisy_pointcloud.T)\n","\n","rot_pointcloud = RandomRotation_z()(noisy_pointcloud)\n","# pcshow(*rot_pointcloud.T)\n","\n","sorted_pointcloud = ToSorted()(rot_pointcloud)\n","# pcshow(*sorted_pointcloud.T)\n","\n","tensor_pointcloud = ToTensor()(sorted_pointcloud)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XAEPalIR6-zn"},"source":["### Creating Loaders"]},{"cell_type":"code","metadata":{"id":"H9J2h8knXku9"},"source":["# # all classes\n","\n","# BATCH_SIZE = 32\n","\n","# def trs():\n","#     return transforms.Compose([\n","#                                 PointSampler(1024),\n","#                                 RandomNoise(),\n","#                                 RandomRotation_z(),\n","#                                 Normalize(),\n","#                                 ToSorted(),\n","#                                 ToTensor()\n","#     ])\n","\n","# train_ds = PointCloudData(path, transform=trs())\n","# valid_ds = PointCloudData(path, valid=True, folder='test', transform=trs())\n","\n","# train_loader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","# valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, drop_last=True)\n","\n","# !mkdir dataloaders\n","# torch.save(train_loader, 'dataloaders/trainloader.pth')\n","# torch.save(valid_loader, 'dataloaders/validloader.pth')\n","# !cp -r dataloaders_32_sorted drive/MyDrive/Thesis/dataloaders\n","\n","# # !cp -r drive/MyDrive/dataloaders ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kogg4Z7aiYuk"},"source":["# # all classes - no ToSorted\n","\n","# BATCH_SIZE = 32\n","\n","# def trs():\n","#     return transforms.Compose([\n","#                                 PointSampler(1024),\n","#                                 RandomNoise(),\n","#                                 RandomRotation_z(),\n","#                                 Normalize(),\n","#                                 ToTensor()\n","#     ])\n","\n","# train_ds = PointCloudData(path, transform=trs())\n","# valid_ds = PointCloudData(path, valid=True, folder='test', transform=trs())\n","\n","# train_loader = DataLoader(train_ds, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","# valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, drop_last=True)\n","# !mkdir dataloaders_no_sort\n","# torch.save(train_loader, 'dataloaders_no_sort/trainloader.pth')\n","# torch.save(valid_loader, 'dataloaders_no_sort/validloader.pth')\n","# !cp -r dataloaders_no_sort drive/MyDrive/Thesis/dataloaders\n","\n","# # !cp -r drive/MyDrive/Thesis/dataloaders/dataloaders_no_sort ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IHEL8L22jVP1"},"source":["# # BEDS\n","\n","# BATCH_SIZE = 32\n","\n","# def trs():\n","#     return transforms.Compose([\n","#                                PointSampler(1024),\n","#                                Normalize(),\n","#                                RandomRotation_z(),\n","#                                RandomNoise(),\n","#                                ToSorted(),\n","#                                ToTensor()\n","#     ])\n","\n","# beds_train_dataset = PointCloudData(path, folders=['bed'], transform=trs())\n","# beds_valid_dataset = PointCloudData(path, folder='test', folders=['bed'], transform=trs())\n","\n","# beds_train_loader = DataLoader(dataset=beds_train_dataset, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","# beds_valid_loader = DataLoader(dataset=beds_valid_dataset, batch_size=BATCH_SIZE, drop_last=True)\n","\n","# !mkdir dataloader_beds\n","# torch.save(beds_train_loader, 'dataloader_beds/trainloader.pth')\n","# torch.save(beds_valid_loader, 'dataloader_beds/validloader.pth')\n","# !cp -r dataloader_beds drive/MyDrive/Thesis/dataloaders\n","\n","# # !cp -r drive/MyDrive/Thesis/dataloaders/dataloader_beds ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2gb5SfSbG9d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637221225475,"user_tz":-180,"elapsed":79125,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"da96df17-9cef-4144-f4e1-7a40551b21af"},"source":["# BEDS \"No Rotation\"\n","\n","BATCH_SIZE = 32\n","\n","def trs():\n","    return transforms.Compose([\n","                               PointSampler(1024),\n","                               Normalize(),\n","                               RandomNoise(),\n","                               ToSorted(),\n","                               ToTensor()\n","    ])\n","\n","beds_train_dataset = PointCloudData(path, folders=['bed'], transform=trs())\n","beds_valid_dataset = PointCloudData(path, folder='test', folders=['bed'], transform=trs())\n","\n","beds_train_loader = DataLoader(dataset=beds_train_dataset, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","beds_valid_loader = DataLoader(dataset=beds_valid_dataset, num_workers=4, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_no_rot_4\n","torch.save(beds_train_loader, 'dataloader_beds_no_rot_4/trainloader.pth')\n","torch.save(beds_valid_loader, 'dataloader_beds_no_rot_4/validloader.pth')\n","!cp -r dataloader_beds_no_rot_4 drive/MyDrive/Thesis/dataloaders\n","\n","# !cp -r drive/MyDrive/Thesis/dataloaders/dataloader_beds_no_noise_rot_4 ./"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"6Vy0JyM7mD9m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637221308093,"user_tz":-180,"elapsed":82642,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"ac8dac12-026c-483d-883b-848f1ba3ad98"},"source":["# BEDS \"No noise\", \"No Rotation\"\n","\n","BATCH_SIZE = 32\n","\n","def trs():\n","    return transforms.Compose([\n","                               PointSampler(1024),\n","                               Normalize(),\n","                               ToSorted(),\n","                               ToTensor()\n","    ])\n","\n","beds_train_dataset = PointCloudData(path, folders=['bed'], transform=trs())\n","beds_valid_dataset = PointCloudData(path, folder='test', folders=['bed'], transform=trs())\n","\n","beds_train_loader = DataLoader(dataset=beds_train_dataset, num_workers=4, shuffle=True, batch_size=BATCH_SIZE)\n","beds_valid_loader = DataLoader(dataset=beds_valid_dataset, num_workers=4, batch_size=BATCH_SIZE)\n","\n","!mkdir dataloader_beds_no_noise_rot_4\n","torch.save(beds_train_loader, 'dataloader_beds_no_noise_rot_4/trainloader.pth')\n","torch.save(beds_valid_loader, 'dataloader_beds_no_noise_rot_4/validloader.pth')\n","!cp -r dataloader_beds_no_noise_rot_4 drive/MyDrive/Thesis/dataloaders\n","\n","# !cp -r drive/MyDrive/Thesis/dataloaders/dataloader_beds_no_noise_rot_4 ./"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Vc_RQyTr78s","executionInfo":{"status":"ok","timestamp":1637221395597,"user_tz":-180,"elapsed":87520,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"aa44481a-1e9d-47d5-f6c7-86e258678dab"},"source":["BATCH_SIZE = 32\n","\n","def trs_small():\n","    return transforms.Compose([\n","                               PointSampler(256),\n","                               ToSorted(),\n","                               RandomNoise(),\n","                               Normalize(),\n","                               ToTensor()\n","    ])\n","\n","beds_train_dataset_small = PointCloudData(path, folders=['bed'], transform=trs_small())\n","beds_valid_dataset_small = PointCloudData(path, folder='test', folders=['bed'], transform=trs_small())\n","\n","beds_train_loader_small = DataLoader(dataset=beds_train_dataset_small, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","beds_valid_loader_small = DataLoader(dataset=beds_valid_dataset_small, num_workers=4, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_small_4\n","torch.save(beds_train_loader_small, 'dataloader_beds_small_4/trainloader.pth')\n","torch.save(beds_valid_loader_small, 'dataloader_beds_small_4/validloader.pth')\n","!cp -r dataloader_beds_small_4 drive/MyDrive/Thesis/dataloaders"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"I39Fc9ZIv1RB"},"source":["class PointCloudDataPre(Dataset):\n","    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms(), folders=None):\n","        self.root_dir = root_dir\n","        if not folders:\n","            folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n","        self.classes = {folder: i for i, folder in enumerate(folders)}\n","        self.transforms = transform\n","        self.valid = valid\n","        self.pcs = []\n","        for category in self.classes.keys():\n","            new_dir = root_dir/Path(category)/folder\n","            for file in os.listdir(new_dir):\n","                if file.endswith('.off'):\n","                    sample = {}\n","                    with open(new_dir/file, 'r') as f:\n","                        verts, faces = read_off(f)\n","                    sample['pc'] = self.transforms((verts, faces))\n","                    sample['category'] = category\n","                    self.pcs.append(sample)\n","\n","    def __len__(self):\n","        return len(self.pcs)\n","\n","    def __getitem__(self, idx):\n","        pointcloud = self.pcs[idx]['pc']\n","        category = self.pcs[idx]['category']\n","        return pointcloud, self.classes[category]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B47a4mMOv67I"},"source":["# BATCH_SIZE = 32\n","\n","# def trs_small():\n","#     return transforms.Compose([\n","#                                PointSampler(256),\n","#                                ToSorted(),\n","#                                RandomNoise(),\n","#                                Normalize(),\n","#                                ToTensor()\n","#     ])\n","\n","# beds_train_dataset_small = PointCloudDataPre(path, folders=['bed'], transform=trs_small())\n","# beds_valid_dataset_small = PointCloudDataPre(path, folder='test', folders=['bed'], transform=trs_small())\n","\n","# beds_train_loader_small_pre = DataLoader(dataset=beds_train_dataset_small, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","# beds_valid_loader_small_pre = DataLoader(dataset=beds_valid_dataset_small, num_workers=4, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_small_pre_4\n","torch.save(beds_train_loader_small_pre, 'dataloader_beds_small_pre_4/trainloader.pth')\n","torch.save(beds_valid_loader_small_pre, 'dataloader_beds_small_pre_4/validloader.pth')\n","!cp -r dataloader_beds_small_pre_4 drive/MyDrive/Thesis/dataloaders"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4liyay_nebO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637233954452,"user_tz":-180,"elapsed":235866,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"4e510884-99f0-4001-eb55-d2e11179cdd9"},"source":["BATCH_SIZE = 32\n","\n","def trs():\n","    return transforms.Compose([\n","                               PointSampler(1024),\n","                               ToSorted(),\n","                            #    RandomNoise(),\n","                               Normalize(),\n","                               ToTensor()\n","    ])\n","\n","beds_train_dataset_pre = PointCloudDataPre(path, folders=['bed'], transform=trs())\n","beds_valid_dataset_pre = PointCloudDataPre(path, folder='test', folders=['bed'], transform=trs())\n","\n","beds_train_loader_pre = DataLoader(dataset=beds_train_dataset_pre, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","beds_valid_loader_pre = DataLoader(dataset=beds_valid_dataset_pre, num_workers=4, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_pre_4\n","torch.save(beds_train_loader_pre, 'dataloader_beds_pre_4/trainloader.pth')\n","torch.save(beds_valid_loader_pre, 'dataloader_beds_pre_4/validloader.pth')\n","!cp -r dataloader_beds_pre_4 drive/MyDrive/Thesis/dataloaders"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cODzYZ6Q2Fuj","executionInfo":{"status":"ok","timestamp":1637606946909,"user_tz":-180,"elapsed":245089,"user":{"displayName":"ann whoorma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg90E06iMwiHMgVlm7l0XgLVeRoC_NenXcVq4DI=s64","userId":"17270067989514599745"}},"outputId":"59c6a9e3-c249-47e4-d9c1-f0c81b88ed8c"},"source":["BATCH_SIZE = 32\n","\n","static_trs = transforms.Compose([\n","                               PointSampler(1024),\n","                               ToSorted(),\n","                               Normalize(),\n","])\n","\n","later_trs = transforms.Compose([\n","                                RandomNoise(),\n","                                ToTensor()\n","])\n","\n","beds_train_dataset_pre = PointCloudDataBoth(path, folders=['bed'], static_transform=static_trs, later_transform=later_trs)\n","beds_valid_dataset_pre = PointCloudDataBoth(path, folder='test', folders=['bed'], static_transform=static_trs)\n","\n","beds_train = DataLoader(dataset=beds_train_dataset_pre, num_workers=4, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","beds_valid = DataLoader(dataset=beds_valid_dataset_pre, num_workers=4, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_both_4\n","torch.save(beds_train, 'dataloader_beds_both_4/trainloader.pth')\n","torch.save(beds_valid, 'dataloader_beds_both_4/validloader.pth')\n","!cp -r dataloader_beds_both_4 drive/MyDrive/Thesis/dataloaders"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Cxx-TQbuR_3q"},"source":["BATCH_SIZE = 32\n","\n","static_trs = transforms.Compose([\n","                               PointSampler(1024),\n","                               ToSorted(),\n","                               Normalize(),\n","])\n","\n","later_trs = transforms.Compose([\n","                                RandomNoise(),\n","                                ToTensor()\n","])\n","\n","beds_train_dataset_pre = PointCloudDataBoth(path, folders=['bed'], static_transform=static_trs, later_transform=later_trs)\n","beds_valid_dataset_pre = PointCloudDataBoth(path, folder='test', folders=['bed'], static_transform=static_trs)\n","\n","beds_train = DataLoader(dataset=beds_train_dataset_pre, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n","beds_valid = DataLoader(dataset=beds_valid_dataset_pre, shuffle=False, batch_size=BATCH_SIZE, drop_last=True)\n","\n","!mkdir dataloader_beds_both\n","torch.save(beds_train, 'dataloader_beds_both/trainloader.pth')\n","torch.save(beds_valid, 'dataloader_beds_both/validloader.pth')\n","!cp -r dataloader_beds_both drive/MyDrive/Thesis/dataloaders"],"execution_count":null,"outputs":[]}]}